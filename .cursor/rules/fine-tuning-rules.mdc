---
description: Rule 3. fine-tuning-rules (Hugging Face-Specific). Rule 3.1 Dataset Preparation for Solana Tasks Curate datasets like tahanmajs/bitcoin-llm-finetuning-dataset (adapt for Solana) with instruction-response pairs (e.g., "Analyze this SOL transaction [data]" → "Whale move detected, 5,000 SOL transferred"). Include perpetuals data (e.g., funding rates from Jupiter) and NFT minting examples. Use 1,000-10,000 examples, balancing tasks to avoid bias. Rule 3.2 Efficient Fine-Tuning Parameters Employ LoRA with r=16, lora_alpha=32, targeting modules like q_proj and v_proj. Set epochs to 3, batch size to 4, learning rate to 2e-4. Use quantization (4-bit) for Solana tasks involving numerical analysis (e.g., open interest trends). Rule 3.3 Evaluation for Domain Accuracy Test on held-out Solana data with metrics like ROUGE for responses and custom accuracy for tasks (e.g., correct whale detection). Iterate if the agent fails actions like balance requests—add Helius API keys for asset queries. Rule 3.4 Integration with Solana Kits Post-fine-tuning, load the model into frameworks like Solana Agent Kit via ChatOpenAI or smolagents. Ensure the agent can stream responses and handle memory (e.g., MemorySaver).
alwaysApply: false
---
